{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7abda90-788b-4ec8-b1ad-993ef5fb3821",
   "metadata": {},
   "source": [
    "# Statistical Inference\n",
    "\n",
    "[HistFactory](https://cds.cern.ch/record/1456844) is a tool to construct probabilty distribution functions from template histograms, constructing a likelihood function. In this exercise we will be using HistFactory via [pyhf](https://pyhf.readthedocs.io/), a python implementation of this tool. In addition, we will be using the cabinetry package, which is a python library for constructing and implementing HistFactory models. At the end pyhf turns the statistical model into likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed30299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "import numpy as np\n",
    "import uproot\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pyhf.set_backend(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal toy workspace specification\n",
    "\n",
    "data_counts = [120.0]\n",
    "bkg_counts = [100.0]\n",
    "sig_counts = [20.0]\n",
    "\n",
    "workspace_spec_toy = {\n",
    "    \"channels\": [\n",
    "        {\n",
    "            \"name\": \"SR\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"name\": \"signal\",\n",
    "                    \"data\": sig_counts,\n",
    "                    \"modifiers\": [\n",
    "                        {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"background\",\n",
    "                    \"data\": bkg_counts,\n",
    "                    \"modifiers\": [],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    \"measurements\": [\n",
    "        {\n",
    "            \"name\": \"measurement\",\n",
    "            \"config\": {\n",
    "                \"poi\": \"mu\",\n",
    "                \"parameters\": [],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"observations\": [\n",
    "        {\n",
    "            \"name\": \"SR\",\n",
    "            \"data\": data_counts,\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0.0\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workspace and model\n",
    "ws_toy = pyhf.Workspace(workspace_spec_toy)\n",
    "model = ws_toy.model()\n",
    "data = ws_toy.data(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aea39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct the fit\n",
    "fit_result = pyhf.infer.mle.fit(data, model)\n",
    "mu_hat = fit_result[model.config.poi_index]\n",
    "print(f\"Fit result (toy): mu = {mu_hat:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77560655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood scan (toy)\n",
    "poi_vals = np.linspace(0., 2., 50)\n",
    "delta_nlls = []\n",
    "\n",
    "init_pars = model.config.suggested_init()\n",
    "par_bounds = model.config.suggested_bounds()\n",
    "fixed_params = model.config.suggested_fixed()\n",
    "\n",
    "# Get minimum NLL at MLE\n",
    "mle_fit = pyhf.infer.mle.fit(data, model)\n",
    "min_nll = -2 * model.logpdf(mle_fit, data)\n",
    "\n",
    "for mu in poi_vals:\n",
    "    test_pars = init_pars.copy()\n",
    "    test_pars[model.config.poi_index] = mu\n",
    "\n",
    "    fixed = fixed_params.copy()\n",
    "    fixed[model.config.poi_index] = True  # Fix POI to scan value\n",
    "\n",
    "    fit_result = pyhf.infer.mle.fit(data, model, init_pars=test_pars, par_bounds=par_bounds, fixed_params=fixed)\n",
    "    nll = -2 * model.logpdf(fit_result, data)\n",
    "    delta_nlls.append(nll - min_nll)\n",
    "\n",
    "# Plot ΔNLL vs POI\n",
    "plt.plot(poi_vals, delta_nlls)\n",
    "plt.xlabel(model.config.poi_name)\n",
    "plt.ylabel(\"ΔNLL\")\n",
    "plt.title(f\"Likelihood Scan for {model.config.poi_name}\")\n",
    "plt.axhline(1.0, color='red', linestyle='--', label=r'68% CL (ΔNLL = 1)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6806fa-692e-4912-bd99-d8928a5f9404",
   "metadata": {},
   "source": [
    "## Introducing cabinetry\n",
    "\n",
    "Out histograms.root file has over 100 histograms. We could use pyhf to create the workspace, but this would result in a very large nested dictionary. This is hard to read (and write). So, instead, we use another tool: cabinetry.\n",
    "\n",
    "A statistical model can be define in a declarative way using cabinetry, capturing the $\\mathrm{region \\otimes sample \\otimes systematics}$ structure. General settings `General`, list of phase space regions such as signal and control regions `Regions`, list of samples (MC and data) `Samples`, list of systematic uncertainties `Systematics`, and a list of normalization factors `NormFactors`.\n",
    "\n",
    "In the `Systematics` section we specify which systematic effects we want to take into account. In addition to the W+jets scale variations, b-tagging variations, and jet energy scale and resolution (shown in the full file) we show here for the ttbar samples `_ME_var` (what does the result look like if we choose another generator?) and `_PS_var` (what does the result look like if we use a different hadronizer?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1e3da-40af-4d7e-8f67-ca8b15ed8107",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Real data from .root file\n",
    "# The .root file contains 146 histograms. We could define a workspace using pyhf,\n",
    "# but that will be a mess. Instead we will use another tool called 'cabinetry'\n",
    "\n",
    "import logging\n",
    "import cabinetry\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)\n",
    "\n",
    "config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9512e-14dd-450c-a841-166165b21c8d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45675780-bad7-4d63-9726-4f634c06ff04",
   "metadata": {},
   "source": [
    "Now we perform our maximum likelihood fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1873fa-f0c2-44f4-b77a-4ed4b4a7bd64",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff98f1-1226-4f23-a375-52f4954d7fab",
   "metadata": {},
   "source": [
    "and visualize the pulls of parameters in the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d6f10-ed62-4126-981e-a5949df488b2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pull_fig = cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=False, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20012da-125b-4422-961c-71d773ce6565",
   "metadata": {},
   "source": [
    "What are pulls? For our nuisance parameters in the fit the pull is defined as $(\\hat{\\theta} - \\theta_0)/\\Delta\\theta$, which is the difference between the fitted parameter value and the initial value divided by the width. Looking at the pulls can aid in seeing how well (or how badly) your fit performed. For unbiased estimates and correctly estimated uncertainties, the pull should have a central value of 0 and an uncertainty of 1. If the central value is not 0 then some data feature differs from the expectation which may need investigation if large. If the uncertainty is less than 1 then something is constrained by the data. This needs checking to see if this is legitimate or a modeling issue.\n",
    "\n",
    "What does the model look like before and after the fit? We can visualize each with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6249feb-f64b-4ce8-bdc5-5a5264e7c497",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=False)\n",
    "\n",
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction_postfit, data, close_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fdb88-3250-4aef-b0b8-46b953198300",
   "metadata": {},
   "source": [
    "We can see that there is very good post-fit agreement. Finally, what’s the $t\\bar{t}$ cross section (for our pseudodata) divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb2db6-f240-40ac-b771-49c852eb6da0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c8c2e-4134-4f8c-a26f-98dcab4980d9",
   "metadata": {},
   "source": [
    "We can also visualize the Negative Log-Likelihood (NLL) scan, which shows how the likelihood varies as a function of our parameter of interest (POI), the $t\\bar{t}$ signal strength. The minimum of this curve corresponds to the most likely value of the POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c682d29-2cbd-48fa-a75c-4e70874b6e8c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "poi_scan = cabinetry.fit.scan(\n",
    "    model,\n",
    "    data,\n",
    "    \"ttbar_norm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e6c7c-7727-495c-bb9b-25957dce3e39",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = poi_scan.parameter_values   \n",
    "y = poi_scan.delta_nlls         \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, marker='o')\n",
    "plt.xlabel(\"POI\")  \n",
    "plt.ylabel(\"ΔNLL\")\n",
    "plt.title(\"NLL Scan\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
